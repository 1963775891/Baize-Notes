![cover_image](https://mmbiz.qpic.cn/sz_mmbiz_jpg/fY8ibThH1At5OKCwshyLbLgyYkUliahWcfeoG0x4K4yDtEbPN0Fc4KzRTykAIZd4J2OYPibqVL2Gla8UIGcMYx9tg/0?wx_fmt=jpeg)

#  生图和训练当中的辞海（其他参数篇）

[ 小鱼干了 ](javascript:void\(0\);)

**小鱼干了**

微信号  ifishhh

功能介绍  不定期推送AIGC指南，设计干货，行业内幕，观点吐槽... ... 这里有好玩的地方，美味的食物，优秀的设计，实惠的旅程，以及不正经的观点...

__ __

__ _ _

  

** 参数词典  **

###  ** overfitting 过拟合  **

Creating a model that matches the training data so closely that the model
fails to make correct predictions on new data.

创建的模型与训练数据过于匹配，以致于模型无法根据新数据做出正确的预测。

###  ** 收敛 (convergence)  **

通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练损失和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。

###  ** VAE  **

Variational autoencoders (VAEs)
是一种用于学习潜在表示的深度学习技术。它们也被用来绘制图像，在半监督学习中取得最先进的成果，以及在句子之间进行插值。

###  ** CFG  **

这个词汇为 Classifier Free Guidance Scale 的缩写，用于衡量模型 生成的预期图片和你的提示保持一致的 程度。Cfg Scale
值为 0 时，会生成一个基于种子的随机图像。

打个比方，想象你的提示是一个带有可变宽度光束的手电筒，你将它照到模型的潜在空间上以突出显示特定区域——你的输出图像将从该区域内的某个地方绘制，具体取决于种子。

将 Cfg Scale 拨向 零会产生极宽的光束 ，突出显示整个潜在空间——您的输出几乎可以来自任何地方。

将 Cfg Scale 拨向 20 会产生非常窄的光束， 以至于在极端情况下它会变成激光指示器，照亮潜在空间中的一个点。

###  ** Loss functions  **

在优化算法的上下文中，用于评估候选解决方案（即一组权重）的函数称为目标函数。

对于神经网络，我们寻求最小错误。因此，目标函数通常被称为成本函数或损失函数，由损失函数计算的值简称为“loss”。

** 损失函数。  **

关于 Loss Function 的中文解释请读 损失函数

###  ** 潜在空间  **

压缩数据的表示，其中相似的数据点在空间上更靠近在一起。5

关于潜在空间的中文解释请读 理解机器学习中的潜在空间.

###  ** 损失  **

一种衡量指标，用于衡量模型的预测偏离其标签的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将均方误差用于损失函数，而逻辑回归模型则使用对数损失函数。6

###  ** Hyperparameter  **

超参数

机器学习算法的参数。示例包括在决策林中学习的树的数量，或者梯度下降算法中的步长。在对模型进行定型之前，先设置超参数的值，并控制查找预测函数参数的过程，例如，决策树中的比较点或线性回归模型中的权重。

###  ** Pipeline  **

要将模型与数据集相匹配所需的所有操作。管道由数据导入、转换、特征化和学习步骤组成。对管道进行定型后，它会转变为模型。

###  ** epoch  **

在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（N/批次规模）次训练迭代，其中 N 是样本总数。

###  ** batch size  **

一个批次中的样本数。例如，SGD 的批次规模为 1，而小批次的规模通常介于 10 到 1000
之间。批次规模在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次规模。

###  ** iteration 迭代  **

模型的权重在训练期间的一次更新。迭代包含计算参数在单个批量数据上的梯度损失。6

###  ** Tensor  **

TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N
可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。

###  ** checkpoint  **

一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型权重，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，图本身不包含在检查点中。

###  ** embeddings  **

一种分类特征，以连续值特征表示。通常，嵌入是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：

  * 表示成包含百万个元素（高维度）的稀疏向量，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。 

  * 表示成包含数百个元素（低维度）的密集向量，其中每个元素都包含一个介于 0 到 1 之间的浮点值。这就是一种嵌套。 

在 TensorFlow 中，会按反向传播损失训练嵌套，和训练神经网络中的任何其他参数时一样。

###  ** 激活函数  **

一种函数（例如 ReLU 或 S 型函数），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。6

###  ** weight  **

线性模型中特征的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。

###  ** ENSD  **

ENSD 是 eta 噪声种子增量，它改变了种子。

NAI 使用 31337

###  ** CLIP  **

CLIP 是一种非常先进的神经网络，可将提示文本转换为数字表示。对一个相对复杂场景的文本描述，AI
需要能“理解”并匹配到对应的画面，大部分项目依赖的都是一个叫 CLIP 的模型。

CLIP 在生成模型的潜在空间进行搜索，从而找到与给定的文字描述相匹配的潜在图像，它非常现代且高效。

** ignore-last-layers-of-clip-model  **

WebUi 使用的是 clip-interrogator 项目，它结合了 blip 和 clip 项目，很大地优化 图像到文本 的过程。blip
负责从原图片中解读文本，由 clip 负责解读适合创作的新图像的描述。

###  ** CUDA  **

配合 CUDA 技术，显卡可以模拟成一颗 PhysX 物理加速芯片。

** 迭代步数和学习率是什  ** ** 么  **

现在AI训练都会采用一个叫"随机梯度下降”的算法，其实这个算法很简单，就是沿着让函数损失最小的方向不断逼近，在Stable
Diffusion训练中就是让生成出来的图片不断朝着我们给的目标图片靠近。

在机器学习中，我们把这样衡量损失大小的东西叫损失函数，损失越小，就是和目标越靠近，比如我们可以用个抛物线来表达损失函数，一开始我们损失在最左边图的红点上，我们就会往谷底"不断下降，所以这个算法叫"梯度下降”

可以看到刚才图三下降了两三次就到了谷底，这个下降次数就是训练的迭代步数。所以如果迭代步数过少，可能还没学会目标信息;过多可能就超过了最佳点，或者朝着学到“过多信息”的地方发展了
(也就是过拟合)

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42XQv6S7ZuHJicbCMiaM70ibHxcKZB7ITo0RgU4SeUmpN5DoYavAfjABveg/640?wx_fmt=png)

|

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42uSibl89icgwLFkiamyNQEibxvzuYW1E8I2xMsp6JfNIhaJI1vO4zJ3xb9w/640?wx_fmt=png)

|
![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42yOa9ZqIpsktdYAWJZgWd7xV9icuD4yt88dZpN9RlFB6sOjClHia7Ujng/640?wx_fmt=png)  
---|---|---  
  
我们也看到图三下降了两三次就到了谷底是因为它下降的快，每一步都"大幅迈进”，所以迭代步数很少。这里每一次下降的幅度就称为"学习率"，也就是每次学到了多少东西的意思。可以看到如果学习率太低，那就需要特别多迭代步数来迭代，学的太慢
(左图) 。但是如果学习率太高，可能一下就超过了最佳点，导致来回震荡 (右图)，:甚至错误越来越大

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia423AqaSYFXgChibvZPDvZGz9tj278diaSYTYypKRDrcBxlBYLGVq1kzTBA/640?wx_fmt=png)

|
![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42RdCMLRM4zyVkB7n4qBqBYyWfu235LNzjkXHKSubmd329g0GOz2fCDg/640?wx_fmt=png)  
---|---  
  
  

** 批数量batch size是什么  **

通常我们在训练stable diffusion模型时还会看到一个参数，batch size。batch
size就是每一次迭代时，有多少张图片同时拿来评估损失函数。

如果一次性把所有图片都拿来评估，一个是速度很慢，还会因为一次性加载太多图片而让你GPU内存不够用。所以就有人说，那就一次只拿一些图片来评估损失，这样就解决这两个问题

但是，从可以想到，每次只去其中一些数据点来评估，肯定会有一些误差，比如下面这样数据点，第一次只取了左半边的，第二次只取了右半边的，肯定会有一些误差导致效果不如全部数据点一起评估的

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42ufqeK2Id1PzFAkFf2hQszSPWIJAUxGWUeoaQ8BzBZOF7vMowduveNA/640?wx_fmt=png)

  

文章参考：

stable diffusion book  |  https://stable-diffusion-
book.vercel.app/GettingStarted/term/  
---|---  
AI绘图红宝书  |  https://z28pynubvc.feishu.cn/  wiki  /wikcnYiF2qaBWVtvVmjq7GjPXpc  
  
声明，文档使用 GFDL 许可。  详细声明请点链接查看

![](https://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At5wtvRxKRkN4GWicE93NRia42mINp8NB5HRDKfsnj48CgOiaReyfq5NjYNzTyq80PiczoianApmUnpTsNA/640?wx_fmt=png)

预览时标签不可点

微信扫一扫  
关注该公众号



轻触阅读原文

![](http://mmbiz.qpic.cn/sz_mmbiz_png/fY8ibThH1At6iciciaKY5WZ4ib8CVibVnVHRJwGj6ksg7fk0tzTMuLPsvptv6zswtKfCLNFwYr9aIBGkjiaYGBWtibwnOQ/0?wx_fmt=png)

小鱼干了







****



****



  收藏

