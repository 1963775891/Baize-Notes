## ControlNet 五大参数

ControlNet 是 Stable Diffusion 最重要的插件，种类繁多，功能复杂，但是真正得其要领的人其实很少。尤其是 ControlNet 主面板的五个参数：权重、引导时机、预处理器分辨率、控制模式、缩放模式（图 1-1）。

下面我们深度解析这几个重要参数，理解并掌握这几个参数的用法之后，可以提高图像提示的出图质量以及准确度。

### 1. 控制模式

控制模式是 ControlNet 最重要的一个参数，用来平衡提示词和 ControlNet 之间的矛盾，同时也是最影响出图质量以及准确度的参数之一。控制模式有以下三种：

- **均衡模式**：默认的控制模式，在提示词和 ControlNet 不一致时，平衡两者。
- **偏向提示词模式**：在提示词和 ControlNet 不一致时，提示词优先。
- **偏向 ControlNet 模式**：旧版本中的猜测模式，通过线稿等预处理图去猜测参考图的内容，猜测内容和提示词不一致时，忽略提示词，更看重 ControlNet 的猜测。

这三种模式用起来有质的区别。均衡模式非常好理解，重点在于另外的两种，偏向提示词和偏向 ControlNet 究竟是怎么个偏向法？

总的来说，主要是有两个方面，一个是生成内容的偏向，一个是生成风格的偏向。

#### **a.  内容上的偏向**

如图 1-2 的参考图和 canny 预处理图，笔者不想要警告框和香烟，所以提示词没写香烟和警告框，而 canny 预处理图是有的。另外参考图的佛头顶部缺失了一块，生成的结果如图 1-3 所示。

- 偏向提示词模式下警告框和香烟都被删除（图 1-3 中间）。
- 均衡模式下只留下警告框（图 1-3 左边）。
- 偏向 ControlNet 模式下香烟和警告框都被尽量保留（图 1-3 右边）。

另外在偏向提示词模式下，连佛头顶部缺失的内容被自动补充完整了，这是此模式很重要的一个作用。这意味着只要完整写好提示词，我们不必提供非常精准的参考图，就可以推动 AI 结合提示词准确地生成图像。

#### b. 风格上的偏向

在不考虑图像内容差异的情况下，我们发现图 1-3 偏向提示词模式下的图像质量最好，均衡模式次之，偏向 ControlNet 最差，这并不是一个偶然的情况，而是风格的偏向造成的差距。

我们可以做一个对比：用一张动漫的参考图（图 1-4），分别生成动漫、真人、迪士尼动画三种风格的图像，观察不同控制模式在不同风格转换的差异。

- 生成图也是动漫时，除了偏向 ControlNet 质量差了一点，三种模式都没有出现问题（图 1-5）。
- 生成图是真人写实风格时，可以明显看到，均衡模式和偏向 ControlNet 模式下，图中的女生的头发依然保持动漫的线条风格，只有偏向提示词模式的头发线条才是真正的真人模型的线条（图 1-6）。
- 生成图是迪士尼风格时，同样均衡模式和偏向 ControlNet 模式下，线条还是保持了动漫特点的线条，只有偏向提示词模式的线条比较正确（图 1-7）。

以上就是偏向提示词模式真正的作用。这也是很多人用 Cnny、Sftedge 等线稿模型怎么弄，都没有真人质感的原因所在，就是因为在参考图和生成图风格不同的时候，没有选择正确的控制模式。甚至有一些控制模型，比如 lineart anime，由于缺少猜测模式，基本只能使用偏向提示词模式。

我们可以看图 1-8，在其他参数完全一致的情况下，lineart anime 在均衡和偏向 ControlNet 时出现了很多伪影和质量不佳的情况，只有偏向提示词模式没有问题。作为对比的 lineart 模型三个控制模式都表现良好。

总结一下，ControlNet 的控制模式有三种：均衡、偏向提示词、偏向 ControlNet。当提示词和 ControlNet 有矛盾时，偏向提示词和偏向 ControlNet 会在内容和画风两个方面表现出差异性。结论如下：

- 相同画风、相同内容：均衡或偏向提示词都可以，尽量不用偏向 ControlNet。
- 不同画风、相同内容：偏向提示词。
- 相同画风、不同内容：偏向提示词。

这种内容上的偏向只能是小小的不同，如果参考图跟你要的图有比较大的不同，比如说笔者希望图 1-9 作为参考图，但是背景改在教室里面，光靠偏向提示词模式是不够的。要做这件事情，你还需要搞明白其他几个参数：引导时机和预处理图分辨率。

### 2. 引导步数

引导步数的参数有两个，一个是介入引导步数，一个是退出引导步数。假设我们的采样总步数是 30 步，设置介入引导步数 0.3，设置退出引导步数 0.8。那么整个 30 步的过程，只有 9~24 步 ControlNet 会发挥作用引导采样的过程，而其余的 1-8 以及 25~30 步 ControlNet 不发挥作用（图 1-9）。

有的时候我们想要人为定一下整个画面的构图，但不想介入过多生成的过程。可以通过以下流程完成。下面，笔者通过几笔线稿完成一个画面的构图操作：

1. 拖进 ControlNet（图 1-10）。
2. 选择涂鸦模型，设置退出引导步数 0.3（图 1-11）。

如此，前 30% 的采样会根据线稿定下大框架，后面 70% 则会在大框架之下，让 AI 自行发挥生成，如图 1-12 所示的效果。

假设你对这个星球的亮度不满意，其他的地方保持不变，那么你可以通过以下流程改变星球的亮度：

1. 锁定种子。
2. 用 Photoshop 把想要变量的地方涂上白色，换成黑白图像，拖进新的 ControlNet 窗口（图 1-13）。
3. 采用亮度模型（brightness），设置权重 0.6，介入引导步数 0.4（图 1-14）。

> **技巧提示**：
>
> Brightness 模型非 ControlNet 官方模型，需前往 huggingface 搜索下载。下载后放入 ControlNet 的 models 文件夹，点击模型选择右边的刷新按钮就可以找到该模型。另外，该模型不需要预处理器，预处理器选择无，把图像做成黑白，白的是亮部，黑的是暗部。

如此，既不会影响前期的采样，结构依然保留，只会根据我们的提示调整亮度。最终效果如图 1-15 所示。

### 3. 预处理图分辨率

预处理图是指我们将参考图拖进ControlNet之后，让预处理器处理之后，出来的那张照底白线的线稿图就叫预处理图。预处理图分辨率就是指处理后的线稿图的尺寸大小。
一般情况下，通常我们会勾选完美像素模式，勾选之后预处理图分辨率会跟目标图像尺寸一致，基本不会有什么问题。预处理图分辨率太低或者太高都不好。**

- **预处理图分辨率太低**

  - 理论上，如果低分辨案预处理图生成高分辨率图片会降低生成的质量。

  - 当像素差达到两倍，比如说256预处理生成512图像，会导致图片锯齿、模糊，但是仍然可以保持结构(图1-18)。

  - 像素差三倍及以上，比如说128、170的预处理生成512的图像，就会开始变得像素化，甚至结构都发生变化(图1-19、图1-220)。

- 预处理图分辨率太高
  - 从图1-19图1-21你不难看出，高分率生成低分辨率影响就小很多，它最多只是会让一些细节上的东西消失不见，但我们从生成的图里看不出来。但是从同一张参考图的不同分辨率的预处理图就非常明显(图1-22)，最右边这张2048的预处理图，明显就是线条少了很多，但是单单通过修改预处理图分辨率并不会很大程度影响生成的效果(图1-21)，它只会对细节方面产生影响。
  - 但是，你可以根据自己的意愿，通过提示词、调整控制模式和引导时机等等组合操作，修改它的背景。

4. ### 缩放模式

   Controlnet这里的缩放模式跟图生图的缩放模式是一模一样的，主要是处理参考图的比例跟输出图像的不一致的情况的，总共有三种方式：仅调整大小、裁剪后缩放、缩放后填充空白。
   我们以图1-22为例，用左边这张openpose图生成右边这张相同尺寸的图的时候，两张图是完美契合的。

### 5. 控制权重

控制权重就是ControlNet的引导强度，这个参数并不复杂。图1-30的参考图还是图1-29，用canny模型引导生成，权重0.2和0.4的时候鞋子和丝袜都没有被引导出来，0.6和0.8的权重鞋子被引导了出来，权重1.0的时候鞋子和丝袜都被引导出来了。使用一张素材作为参考图，然后通过用ControlNet换背景、换前景以及做局部修改生成如图1-30所示的效果。

任务1；用1-30的参考图生成一张机甲少女图，要求黑色头发，场景为教室。

任务2：生成效果1作为参考图生成一张二次元的图，要求保持微笑，穿着旗袍，露出手臂。

任务3：在任务2效果基础上给女生带上一顶帽子，且手臂抬起。(注意这里是给该女生带上帽子，而不是作为参考图，要求人物不能改变。)

#### **任务1 制作流程**

1. 提示词中加入教室、黑色头发等提示词。将参考图导入Controlnet，选择canny模型，偏向提示词模式，把Canny的阈值调高，阈值越高，线稿就越简洁，再把预处理图分辨率调高，调到背景的线不怎么明显就可以了，留下点无所谓(图1-31)。如此就可以得到图1-32所示的效果。
2. 新开一个ControlNet，将图1-33导入新的ControlNet，选择局部重绘，给红圈范围涂上涂鸦。
3. 回到Canny窗口，将引导终止时机调整到0.3(图1-34)。如此，就可以将背景彻底换成教室背景(图1-35)。

总的来说，上面我们就是用了两个办法消除背景线条的影响：

- **第一步**：通过调整分辨率和阈值，在保持人物线条的基础上，消除大部分的背景。
- **第二步**：通过减少引导步数，消除第一步遗留下来的局部的顽固的背景线条的影响。

为什么不直接一步到位，这是因为ControlNet一个非常灵活的处理方式：局部参数。

> **知识课堂：局部参数**
>
> 我们通过调整预处理图分辨率还有canny的阈值，已经消除了大部分的背景，只剩下局部比较顽固的线条，强行减少整个引导步数，会直接导致其他的地方失去引导能力。
>
> 这种情况下，我们先让整体先去掉大部分背景，再用局部重绘在比较顽固的线条的位置涂鸦一个局部的窗口，再把canny的引导终止时机提前，就可以在保持其他位置有足够强的引导的情况下，削弱局部的引导。
>
> 这种操作造成的直接结果，就是一张图上面可以同时存在几种不同的参数(图1-36)。

#### **任务2 制作流程**

1. 将生成效果图1导入ControlNet选择tile模型，down sample rate提到4，选择偏向提示词模式如图1-37所示。(这里选择偏向提示词模式有两个原因：①是因为我们不要参考图的机甲，所以必须选择偏向提示词模式②是这一步需要完成转换风格，偏向提示词也是更好的选择。)

反观退出引导步数，即使退出的时间很早，其控制程度也不会受到非常大的影响(图1-17)。

这也反应了Stable diffusion在采样的过程中，前面几步的重要性要远远高于后面的采样。



> 如果笔者用这张768x1024(3:4)的openpose图引导生成一张768x1536(1:2)的图三种模式有什么区别?
>
> - **仅调整大小**：将3：4的比例强行压扁成1：2，内容不会增加也不会减少，仅改变比例(图1-23左边)；
> - **裁剪后缩放**：openpose图会被裁剪成1：2的比例，再引导生成768x1536的图像，内容会有所缺失，如图1-24中间的图手掌被裁剪掉了。
> - **缩放后填充空白**：AI会在768x1024的图像的两端填充到768x1536，内容不仅不会缺失，还会增加新的内容(图1-24右边的图像顶部填充了月亮，底部填充了路面)。

缩放后填充空白是一个非常好的扩展画幅的方式。如图1-24是一张1：1(2048x2048)的图，现在笔者将上下扩展一下扩展成2:3 (768x1024) 的比例。

​	a.导入图片到ControlNet选择局部重绘，预处理器选择inpaint only+lama，不涂任何涂鸦(图1-25)。

​	b.用WD1.4标签器反推出提示词(图1-26)，发送到文生图。

​	c. 输入图片尺寸768x1024(图1-27)，选择一个符合风格的大模型。

如此就可以扩展成一张2:3(768x1024)的图像(图1-28)。

同时我们从这个案例也可以看出来Stable diffusion的一个非常重要的特点，用一张高清的2K图像，去生成一张768x1024的图像，虽然内容得到扩展，但是整体的质量下降了，即使是原图的内容也无法保持原图的质量。这是因为AI绘图并不是在原图的基础上给你填充，它是整张图都重新生成，这就导致扩展后的图像分辨率和质量均有所下降，无法保持原图的质量。

所以正确的流程非常重要，在所有的有不同尺寸需求的任务里面，高清修复都应该放在最后一步。以这个案例来说，核心部分是中间的人，所以我们需要先完成中间768x768的部分，得到满意的图之后再去扩展成768x1024，最后再将768x1024的图进行高清修复，再截图中间的部分，以用于不同的场景。

> **Down Sample Rate的理解**
>
> Down sample rate可以理解为参考图缩小倍数。假设参考图分辨率为768：
>
> - Down sample rate 为1，预处理图分辨率为 768
> - Down sample rate 为2，预处理图分辨率为 384（768的1/2）
> - Down sample rate 为8，预处理图分辨率为 96（768的1/8）

> **为什么要降低预处理图的分辨率？**
>
> Tile模型的特点是将图片切分成小块，然后将每个小块的内容与提示词匹配。降低分辨率是为了让Tile有足够的发挥空间，消除参考图的细节，重构新图像的细节，以达到修改细节的目标。

2. 切换到二次元的模型，在提示词中添加旗袍、赤裸的手臂、微笑等提示词，如图138所示，生成出来的图我们发现她的手臂还是有可能会有手套，如图1-39所示，为了防止ControlNet将参考图中的手识别成手套，应该在反向提示词中补充手套等提示词，如图1-38的反向提示词框。最终得到的效果如图140所示。

**任务3 制作流程**

a.将图1-40导入ControlNet,选择IP-Adapter,预处理器Ip_adapter_clip sdl5,模型Ip_adapter_clip_sdl5plus,控制权重取0.5~0.7，如图所示。

> **疑难问答：为什么IP-Adapter权重取那么小？**
> IP_Adapter是很强势的图像提示词，也就是纯用图像提示词来引导生成，没有任何文字提示词的时候，权重可以取1：如果IP_Adapter和提示词均要发挥作用的时候，权重取0.5~0.8，太高的权重会导致文字提示词几乎发挥不了作用。

b. 将图1-40 导入controlnet，选用 openpose，点击编辑按钮，如图 1-42 所示

c. 在controlnet里面拖动节点就可以编辑姿势，我们把她的左手抬起来，点击发送到controlnet,如图1-43所示。

d. 回到openpose主板，选择dw-openpose-full,这里的控制模式都可以用，如果生成出来的图有明显的不受openpose控制的话，应该选择均衡或者偏向controlnet模式，我这里用偏向提示词模式并没有问题，如图1-44所示。

e. (4)将图1-40导入第三个controlnet,选用局部重绘，模型选择inpaint,.因为出来左手抬起和加帽子之外，其他身体部位都不能动，所以我们需要在左手和头顶涂上涂鸦，并且顶处理器选择inpaint olny,如图1-45所示。

> **疑难问答：局部重绘的预处理器为什么一定是inpaint only才行，inpaint global harmonious不行吗？**
> inpaint only会保证除了涂鸦的部位之外其他位置不发生任何改变，inpaint global harmonious是全局融合算法，他除了会修改涂鸦的内容之外，还会协调其他部位，令其他位置也放生发生改变，显然不符合任务3不改变人物的要求。

f. 在提示词中补充太阳帽、举起手等提示词，由于P-Adapter不是一个十分稳定的模型，所以每当用到IP-Adapter都会比较考验手气，一般进行个三五次，基本就可以得到一个比较准确的图，如图1-46所示。

------

## ControlNet全解析

#### **Lineart线稿**

Lineart是最好用的线稿类模型，有5个预处理器和2个模型：

> **预处理器：**Lineart anime、Lineart anime denoise、Lineart realistic、Lineart coarse、Linear standard
>
> **模型：**Lineart anime.pth、Lineart.pth

1. ##### **预处理器分类与特点**

- **通用线稿预处理器：**Lineart realistici和Lineart coarse，两者区别就是Lineart coarse比较粗糙，更像手绘的线稿，但两者对于不同类型的图像都能有不错的处理，如图3.2.3-2~3.2.3-10，因此都是通用的预处理器。

- **动漫线稿预处理器：**Lineart anime和llineart anime denoise，lineart anime denoise的线稿更干净，也让提示词有更多的发挥空间。这两个预处理器虽然也可以处理其他风格的参考图，如图图3.2.3-11~图3.2.3-19，但是会导致信息缺失，如图3.2.3-11~16.

- **标准处理器：**Lineart standard适合用来处理白底黑线的手绘线稿，如图3.2.3-20，处理图像能力比较一般，如图3.2.3-21~24

2. ##### **模型分类与特点**

- **Lineart.pth（通用线稿模型）：**

​	**a.** 匹配所有风格的checkpoint模型

​	**b.** 不匹配动漫线稿预处理器(Lineart aniem和Lineart annime denoise),因为动漫预处理器(Lineart anime和lineart anime denoise)处理图片会产生一些一个一个的小方格，这些小方格会被通用线稿模型(lineart.pth)还原出来，这些是错误的。

- **Lineart anime.pth（动漫专用）**：

​	专门用于动漫风格，动漫预处理器对非动漫参考图处理会缺失细节，再去掉一条预处理器和参考图不匹配的，剩下7条。(如图，线稿细节损失比较大)

> **注意：**动漫线稿模型没有猜测模式，只能选择**偏向提示词模式**，但也会因此削弱controlnet的控制程度，所以同时，使用动漫线稿模型需要非常详细的提示词。

3. **特殊用法——参考图与需求非常不一致的情况**

- 假设我们的参考图跟我们要的图内容上并不一致，只想用来固定一下框架，然后通过修改提示词生成你想要的东西。
- 这时可以用这个特殊的流程：**非动漫参考图+动漫预处理器+动漫线稿模型+非动漫大模型**
- 选择偏向提示词模式，只要在提示词中写清楚人物和环境的描述，比如这里加一个裙子的提示词，把退出引导步数往前提一点，他就会给你生成你需要的图。

> **其原因如下**：
>
> 1. 偏向提示词模式削弱了controlnet的关注度，导致机甲的内容被削弱。
> 2. Lineart anime.pth是专门用来服务动漫风格checkpoint模型，与写实风格的checkpoint模型配合比较差。
> 3. Lineart anime.pth没有猜测模式，所以不会识别出参考图的机甲的内容。
> 4. Lineart anime预处理器对非二次元参考图的识别缺失信息。

更多的实验结果如下：

------

#### **Scribble涂鸦**

涂鸦功能是比较开放的线稿控制类型，有三个预处理器和一个模型：

- **Scribble hed和Scribble pidi：**是比较粗略的涂鸦，通过控制预处理图分辨率控制涂鸦的精细程度。
- **scribble xdog：**拖动阈值就可以控制涂鸦的精细程度，使用比前面两个方便一点。(图3.2.4-7~图3.2.4-14)

> **控制模式偏向提示词，这个可以用来生成一些脑洞大开的图像。**

------

#### **Softedge软边缘：最严格的边缘锁定**

Softedge模式有四个预处理器：

SoftEdge HED、SoftEdge PIDI、SoftEdge HED safe、SoftEdge PIDI safe，没有白底黑线反色的选项。他们最大的差别是稳定性以及质量的差距，当然对于一般的图像，结果差距很小。

- **最大结果质量排序：**SoftEdge HED > SoftEdge PIDI > SoftEdge HED safe > SoftEdge PIDI safe
- **稳定性排序：**SoftEdge HED < SoftEdge PIDI < SoftEdge HED safe < SoftEdge PIDI safe

------

#### **Canny硬边缘**

Canny模型只有一个预处理器。调整阈值可以控制线稿的细节程度：
- 阈值越低，被识别的边缘越多，细节越多，但错误也越多
- 一般保持默认的100~200即可

------

#### **MLSD直线**

MLSD只能处理直线，无法处理曲线。适合用于生成建筑外立面图等直线较多的场景。

------

### Openpose姿态检测

Openpose是姿态检测模型，通过识别参考图中人体的各个部位来控制图像中人物的肢体动作。

#### **1. 预处理器种类**

- OpenPose：眼睛、鼻子、眼睛、脖子、肩膀、肘部、手腕、膝盖和脚踝。
- OpenPose_face：OpenPose + 面部细节
- OpenPose_hand：OpenPose + 手和手指
- OpenPose_faceonly：仅面部细节
- OpenPose_full：以上所有
- DW_openPose_full：二阶蒸馏算法，OpenPose full增强版

目前最新和最好的预处理器是**DW_openPose_full**。

#### **2. Openpose编辑器的使用**

1. **修改局部动作：**但是按该方法生成人物就发生了变化，如何保证除了修改的部分其他部位不动？（图3.2.8-10、图3.2.8-11)
   我们需要用到inpaint/局部重绘制，把需要重绘的图拖进来，再需要重绘的位置涂上蒙版，选择inpaint only,(图3.2.8-12～图3.2.8-15)
2. **制作人物多角度立绘：**首先制作一张单人舞多角度的openpose图，放进controlnet，预处理器选择无。然后加载一个embending:charturnerv.2(图3.2.8-17)，权重调到1.2。
3. **最后提示词框内输入：**multiple views of the same character in the same outfit以及人物描述，如此可以生成出来这张多视角的人物设计图

------

### **Tile分块**

Tile模型是Stable Diffusion中最强大的模型之一，可以将参考图分成多个小块并识别每个小块的内容。

**主要功能**

1. 辅助高清修复
2. 添加、替换或重新生成图像内容或细节
3. 修复其他超分辨率方法生成的图像
4. **将不同风格素材转换成统一风格**

**预处理器选项**

1. **Blur gaussian(模糊)：**用于调整**景深**，调整参数sigma控制模糊程度，值越高，图片越模糊。
2. **Tile colorfix(固色+布局)：**保持图片布局和颜色，保持原图页面布局和补充原图细节。
3. **Tile resample(仅布局)：**保持图片布局，对原图进行重采样，但会对颜色进行变化，适用于内容丰富的图片。

#### **案例 1：用Tile辅助高清修复**

Tile模型的特点就是他会将图片切分成一个一个的小块，然后将每个小块的内容跟提示词做匹配，如此，即使重绘幅度很高的情况下，整体的结构内容也会被牢牢锁定，避免产生很多不必要的东西。
而tile模型还可以结合Tiled Diffusion生成超高分辨率图片，并且补充了必要的细节。

a. 首先生成一张人物多角度立绘效果图，该图的生成过程详3.2.8的人物多角度立绘案例。

b. 开启controlnet，勾选上传独立的控制图像，把多视角openpose预处理图复制进来，预处理器选择无，模型选择openpose。

c. 接着再启用一个新的controlnet，选择tile模型

d. 打开tiled diffusion，勾选保持输入图片尺寸，选择R-ESRGAN 4x+ Anime6B放大算法，填入放大倍数，本案例是3倍。打开tiledvae。

e. 点击生成即可生成一张高清的多视角人物设计图。

#### **案例 2：修复损坏图像**

Tile控制类型其中一个功能是修复损坏的图像，一张压缩成64x64的柴犬的图片用tile进行了修复，导入这张损坏的图之后，选择Tile模型，提示词写上dog,即可修复这张损坏的图像，但并不能100%还原。



#### **案例 3：转换风格**

第5章介绍过图生图切换风格的方法，切换风格要求重绘幅度在0.6左右才会有比较好的转化效果，但是0.6的重绘幅度要求你的提示词要足够精准，否则就会凭空出现一些其他的东西。如图x,提示词之输入1gil,出现了一个类似头盔的东西。

这种情况下，我们只需要加载一个tile,选择更偏向controlnet模式，这种模式下可以看到就连身体上的纹路都是对的上的，但其他控制模式效果同样非常不错。

如果有局部没有照顾到的，比如说头发的颜色不对，tile暂时无法控制，可以再提示词追加头发的颜色即可。

那么由此衍生出来的另一个用法，就是将一张集合各种风格元素的图像，放进tile模型，选择偏向提示词模式，就可以转换成一个统一风格的图像。



#### **案例 3：修改细节（替换内容）**

Tile模型的替换内容或者修改细节的功能是Tile模型的真正强大之处。
如图x,最左边是原图，重绘幅度调到1.0，开启Tile。

可以看到经历过tile引导的图，身上的盔甲在细节上发生了肉眼可见的改变，由之前光滑的质感变成了比较有颗粒感的质感，肩膀这里变成了比较透明的材质。



并且，在不同down samplerate下，会有不同程度的细节变化，如图xxx，downsamplerate越高细节变化越明显。



down sample rate解释过来大概就是参考图缩小倍数。

假设参考图分辨率为768
down sample rate为1，那么预处理图分辨率则为768
down sample rate为2，那么预处理图分辨率则为768的一半，也就是384
down sample rate为8，那么预处理图分辨率则为768的1/8，也就是96

把预处理图分辨率调低是为了让给tl有足够多的发挥空间，将低分辨率就是消除参考图的细节，重构新图像的细节，以此达到修改细节的目标，所以down sample ratei越大，被修改的细节就越多。
同时偏向提示词模式下的细节变化幅度要比另外两个控制模式要明显很多。

------

### Inpaint局部重绘

Inpaint模型用于局部重绘，可以修改局部内容、扩展画幅，并在一张图像中设置多种参数。

#### **预处理器类型**

- **Inpaint_global_harmonious(改变全图)：**这种预处理器可以提高图像修复或填充的全局一致性，也就是让生成的区域和原始图像的风格、色彩、细节等更加协调。
- **Inpaint_only(仅蒙版)：**这种预处理器只会对用户遮罩或指示的区域进行图像修复或填充。
- **Inpaint_only+lama(先填充,再处理蒙版)：**这种预处理器在进行图像修复或填充之前，会先使用lama模型对图像进行处理。

> inpaint only+lama导入需要扩展的图像，缩放模式选择**「缩放后填充空白」**即可用于**「扩展画幅」**。(图3.2.9-4~图3.2.9-6)

#### **把黑色盔甲修改成旗袍**

1. 局部重绘，提示词加上：旗袍，把脸涂上蒙版，保证脸部不变，你也可以把头发也涂上
2. 选择重绘非蒙版区域，重绘幅度改为 1 。
3. 加载一个tile模型，选择偏向提示词模式。
4. 如此，就可以如愿将原图的盔甲换成了旗袍。

------

### Instruct P2P图片指令

Instruct P2P是一种通过自然语言指令来编辑图像的技术。

使用方法：上传图片；选择Instruct P2P模型；使用"Make it xxx"格式的提示词来指导图像编辑

例如：
- "**Make it fire**"：让图像元素着火
- **"Make she cry**"：让图像中的人物哭泣
- "**Make it on fire**"：将背景变成火焰

------

## IP-Adapter

IP-Adapter 是腾讯 AI 实验室发布的一个新的 stable diffusion 适配器。IP-Adapter 将输入的图像作为图像提示词，类似于 Midjourney 和 DALLE 的垫图。可以用于复制参考图像的风格、构图或人物特征，也可以通过指令修改参考图的局部。

可以这么说，IP-Adapter 是 AnimateDiff 做 AI 动画的人物一致性动画的关键工具。

### 1. ip-adapter预处理器和模型种类

IP-Adapter 中使用两个图像编码器：OpenClip ViT H14（632M 参数）和 OpenClip ViT BigG 14。

- SDXL 的 IP-Adapter 有两个版本，一个采用 OpenClip ViT BigG 14 训练，另一个采用 OpenClip ViT H14 训练。

另外有专门用来处理复刻脸部的 IP 适配器：IP-Adapter Face ID 则使用 Insightface 图像编码器。

#### **SD1.5 的 IP-Adapter**

1. **ip-adapter_SD15.bin：**
   - 使用来自 OpenCLIP-ViT-H-14 的全局图像嵌入作为条件
   - 它大致遵循参考图像的内容，能够捕捉到图像整体的宏观特征。包括颜色特征、纹理特征和形状特征，整体亮度、对比度等。


2. **ip-adapter_SD15_light.bin：**

   - 架构与 ip-adapter_SD15 相同。

   - 更适用于文本提示，它对于使用文本作为提示的创作任务更加灵活。


3. **ip-adapter-plus_SD15.bin：**

   - 使用来自 OpenCLIP-ViT-H-14 的图像块嵌入作为条件。

   - 生成的图像更接近原始参考，但通常无法复刻参考图细节，例如脸部特征。


4. **ip-adapter-plus-face_SD15.bin：**

   - 架构与 ip-adapter-plus_SD15 相同。

   - 模型的权重经过微调，以使用裁剪后的脸部作为参考，因此生成的人物脸部会更像参考图。


#### **SDXL1.0 的 IP-Adapter**

1. **ip-adapter_SDxl.bin：**

   - 使用来自 OpenCLIP-ViT-bigG-14 的全局图像嵌入作为条件。

   - 它大致遵循参考图像的内容，能够捕捉到图像整体的宏观特征。包括颜色特征、纹理特征和形状特征，整体亮度、对比度等。


2. **ip-adapter_SDxl_vit-h.bin：**

   - 使用来自 OpenCLIP-ViT-H-14 的全局图像嵌入作为条件。

   - 与 ip-adapter_SDxl.bin 相同，它大致遵循参考图像的内容，能够捕捉到图像整体的宏观特征。


3. **ip-adapter-plus_SDxl_vit-h.bin：**

   - SD1.5 使用来自 OpenCLIP-ViT-H-14 的图像块嵌入作为条件

   - 比 ip-adapter_xl 和 ip-adapter_SDxl_vit-h 更接近参考图像。


4. **ip-adapter-plus-face_SDxl_vit-h.bin：**

   - 架构与 ip-adapter-plus_SDxl_vit-h 相同

   - 以使用裁剪后的脸部作为参考，因此生成的人物脸部会更像参考图。


#### **Face ID**

1. **IP-Adapter Face ID：**

   - IP-Adapter Face ID 是一个实验性的模型，它使用人脸识别模型中的人脸 ID 嵌入，而不是 CLIP 图像嵌入。

   - 使用 LoRA (ip-adapter-faceid_SD15_lora.safetensors) 来提高 ID 的一致性。IP-Adapter Face ID 可以根据仅有的文本提示生成与人脸相关的不同风格的图像。


2. **IP-Adapter Face ID Plus：**

   - IP-Adapter Face ID Plus 是 IP-Adapter Face ID 的升级版，它更好地保持人物面部的一致性，并能实现与 Reactor 相似的人物换脸效果。

   - 这个模型使用了 face ID 嵌入（用于人脸 ID）以及 CLIP 图像嵌入（用于人脸结构）。

   - 使用了 LoRA (ip-adapter-faceid-plus_SD15_lora.safetensors) 来提高 ID 的一致性。


3. **IP-Adapter Face ID Plus v2：**
   - IP-Adapter Face ID Plus 的改进版，除了改进的模型检查点和 LoRA，还允许设置 CLIP 图像嵌入的权重。


4. **IP-Adapter Face ID SDXL：**
   - 跟 IP-Adapter Face ID 一样使用人脸识别模型中的人脸 ID 嵌入，同样需要使用 lora (ip-adapter-faceid_SDxl_lora.safetensors)，但使用与 SDXL 的 checkpoint 模型。


5. **IP-Adapter Face ID Plus v2 SDXL**

   - 是 IP-Adapter Face ID Plus v2 的 SDXL 版本，它进一步提高了人物面部的一致性，并具备可控的人物换脸效果。

   - FaceID 嵌入（用于人脸 ID）：从人脸识别模型中提取的面部特征。

   - 可控的 CLIP 图像嵌入（用于人脸结构）：用户可以通过权重来调节脸部结构对图像生成的影响程度。

   - LoRA 技术：用于提高 ID 的一致性。


#### **如何选择适配器**

面对这么多的适配器，究竟我们应该怎么选择？

首先 IP-Adapter 作为一种图像提示，是为了弥补文字提示的不足，或者取代文字提示，也就是说图像提示和文字提示之间既有合作关系，也有对抗关系。

所谓合作关系就是文字提示和图像提示匹配，对抗关系就是文字提示和图像提示不匹配。

俺么，我们先撇开 SDXL 的适配器，先把 SD1.5 的所有适配器弄清楚，SDXL 也就自然而然了。

SD1.5 的适配器分成三种，

**第一种：ip-adapter SD1.5、1.5plus、1.5light**，都是 OpenClip VīT H14 图形编码器全图图像嵌入，区别就在于

> SD 1.5：它大致遵循参考图像的内容，能够捕捉到图像整体的宏观特征。

当提示词匹配的时候，表现良好，除了手的动作和脸部不像之外，其他都没有什么问题；

但是提示词跟图像不一致的时候，比如我这里蓝色头发，跑步，那么，权重过低的时候，图像提示词的特征就会被忽略，生成的图就会变成蓝色的头发，但仍然没有看到跑步的动作，所以整体结构还是保留了参考图的结构。

> SD 1.5light 更适用于文本提示，它对于使用文本作为提示的创作任务更加灵活

我们从这二个图就可以看出，图像提示甚至对整体结构都没有做固定，只有人像的大特征做保留了下来，换到蓝色头发和跑步的提示词，权重低的几乎是倾向提示词了，参考图的特征几乎没有了~

> SD 1.5 plus 生成的图像更接近原始参考，但通常无法复刻参考图细节，例如脸部特征。

采用 plus 的时候，权重稍高一点，除了脸部不太像，人物大特征、结构动作动几乎一致，换到蓝色头发和跑步，结构依然稳固，动作也追随参考图，仅仅在权重比较低的时候，出现了一些蓝色的头发~

总结下来就是：对于图像提示的强度：plus 大于 SD1.5 大于 light，plus 适合偏向参考图的需求，light 适合偏向提示词的需求，而 SD1.5 适合在偏向参考图的基础上，做一些细小的变化。

**第二种：SD1.5 plus face，SD1.5 full face**，也是 OpenClip ViT H14 图形编码器**，但是模型的权重经过微调，以使用裁剪后的脸部作为参考，因此生成的人物脸部会更像参考图。从效果上来说 PLUS FACE 和 full face 可以说是难分伯仲，但是都不算特别好。所以在 FACE ID 面世之后，这两个模型变得非常鸡肋。

**第三种：face id，IP-Adapter Face ID** 使用人脸识别模型中的人脸 ID 嵌入，并且使用 **LoRA** (ip-adapter-faceid_SD15_lora.safetensors) 来提高脸部的一致性，lora 的权重 0.5~1.0 都可以。

Face ID 的效果是可以的，但是总体来说不算特别稳定，因为 face id 缺少脸部结构的支持。

**IP-Adapter Face ID Plus** 是 IP-Adapter Face ID 的升级版，plus 模型除了使用了 face ID 嵌入（用于人脸 ID）以及 **CLIP 图像嵌入**（用于人脸结构）。使用了 LoRA (ip-adapter-faceid-

**Plus V2 是 plus 的进化版**，除了改进的模型检查点和 LORA 还允许调整 CLIP 图像权重 r，也就是脸部结构的权重，但只能在 comfyui 用。

IP-Adapter-FaceID-Portrait：与 IP-Adapter-FaceID 相同，但它不需要 lora，它接受多个面部图像以增强相似性。你可以在 controlnet 里面的多张上传这里上传五张图像，进行生成。

事实上 faceid、faceid plus 等等都可以不用 lora，只不过使用 lora 效果会更好。

接下来我们来讲点有趣的应用，首先是权重的影响

### 2. 权重的影响

- **无提示词，权重取 1**

不同的权重将引导至不同的结果，在没有其他提示词的情况下，权重可以直接取 1，权重越大，越倾向于参考图片。

- **有提示词，权重取 0.5~0.8**

我们可以通过补充提示词，在图像提示词的基础上，增加元素或者替换元素，比如说这里给他戴个帽子，补充提示词：wearing a hat，当然如果你要提示词发挥作用，权重就变得尤为重要，0.5 的权重是比较理想的选择，权重大了没有帽子，权重小了跟参考图不像。

### 3. 与其他 controlnet 模型一起使用

IP-Adapter 通常不会单独使用，一般都跟其他的 controlnet 共同引导。

- 与 softedge 模型共同引导
- 与tile模型共同引导
- 与openpose共同引导

### 4. 人物一致性动画

那么，既然图片可以将参考图绑定到 openpose 上面，那视频也必然是可以，操作十分简单，我们不过就是将预处理图的序列帧做成视频，导入到 animatediff 中，controlnet 采用 openpose，预处理器选择无。再开启一个 controlnet，选择 ip-adapter plus，就可以制作一致性人物动画。

------

## Reference

Reference 是 Stability AI 公司发布的官方模型

- **Reference** 是一种基本的方法，用于将参考图像的风格或内容迁移到生成的图像中。
- 它可以用于传输颜色、风格或其他特征，但通常不如其他方法（如 IP-Adapter）强大。
- 适用于简单的风格迁移或颜色转移。

同样，正常情况下 reference 不会单独使用，而是结合其他 controlnet 模型共同引导。

图 xxx 是 reference 和 canny 共同引导生成，图 xxx 是 reference 和 tile 共同引导生成。

### 1. 预处理器种类

`reference` 不使用控制模型，在选择了参考预处理器后，模型下拉菜单将被隐藏。`reference` 有三个预处理器，分别是：

- **Reference adain**: 

  这个预处理器用于风格迁移，通过自适应实例标准化（Adaptive Instance Normalization，AdaIN）来实现。可以将图像的风格从参考图像传递到生成的图像中。

- **Reference only**:

  用于直接生成与参考图像相似的图像，将参考图像直接链接到注意力层。

- **Reference adain+attn**:

  结合了 AdaIN 和注意力机制（Attention）。可以更精细地控制生成图像的风格和细节。

从结果上看，`reference only` 最接近参考图，`reference adain+attn` 次之，`reference adain` 跟参考图最不相符。

### 2. 控制模式的影响

对于 `reference` 模型来说，只有在偏向 `controlnet` 和均衡模式下，生成图才有几分参考图的特性，如图 xxx，从头发的线条就可以判断个大概。

### 3. 风格忠诚度的作用

风格忠诚度是 `reference` 可调节的唯一一个阈值，均衡模式下风格忠诚度才会起作用，其他两个模式即使可以调节，也不起作用。风格忠诚度越高，越接近参考图。如图 xxx。